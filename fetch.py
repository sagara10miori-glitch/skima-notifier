import requests
from bs4 import BeautifulSoup
import re
import time

URL = "https://skima.jp/dl/search"

# å®‰å…¨ãªä¸€èˆ¬ãƒ–ãƒ©ã‚¦ã‚¶ã® User-Agentï¼ˆå½è£…ã§ã¯ãªãäº’æ›æ€§ã®ãŸã‚ã®è¨­å®šï¼‰
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
}


def fetch_items(priority_only=False):
    html = None

    for attempt in range(2):
        try:
            r = requests.get(URL, headers=HEADERS, timeout=10)
            if r.status_code == 200:
                html = r.text
                break
            print(f"[WARN] fetch status={r.status_code}")
            time.sleep(2 + attempt)
        except Exception as e:
            print(f"[WARN] fetch exception: {e}")
            time.sleep(2)

    if not html:
        print("[WARN] failed to fetch item-list")
        return []

    soup = BeautifulSoup(html, "lxml")
    items = []

    # æ–°ã—ã„ SKIMA ã®å•†å“ã‚«ãƒ¼ãƒ‰æ§‹é€ 
    for li in soup.select("li"):
        inner = li.select_one(".inner")
        if not inner:
            continue

        # ç”»åƒ
        img_tag = inner.select_one(".image img")
        image = img_tag.get("src") if img_tag else None

        # ä¾¡æ ¼
        price_tag = inner.select_one(".price")
        price_text = price_tag.get_text(strip=True) if price_tag else "0"
        price = int(re.sub(r"\D", "", price_text) or 0)

        # ã‚¿ã‚¤ãƒˆãƒ«
        title_tag = inner.select_one("h5 a")
        title = title_tag.get_text(strip=True) if title_tag else "ä¸æ˜"

        # URL
        url = "https://skima.jp" + title_tag.get("href") if title_tag else ""

        # ä½œè€…
        author_tag = inner.select_one(".username a")
        author_name = author_tag.get_text(strip=True) if author_tag else "ä¸æ˜"

        author_id = None
        if author_tag:
            href = author_tag.get("href") or ""
            if "id=" in href:
                author_id = href.split("id=")[-1]

        # ãƒ©ãƒ³ã‚¯ï¼ˆSKIMA æ–°UIã§ã¯å­˜åœ¨ã—ãªã„å¯èƒ½æ€§ã‚ã‚Šï¼‰
        rank = "é€šå¸¸"

        # æ·±å¤œå¸¯ãƒ•ã‚£ãƒ«ã‚¿
        if priority_only and rank not in ("ğŸ”¥ç‰¹é¸", "âœ¨ãŠã™ã™ã‚"):
            continue

        items.append({
            "id": url.split("=")[-1],  # detail?id=xxxx ã‹ã‚‰å–å¾—
            "title": title,
            "price": price,
            "author_id": author_id,
            "author_name": author_name,
            "rank": rank,
            "image": image,
            "url": url,
        })

    print(f"[INFO] fetch_items: {len(items)} items (priority_only={priority_only})")
    return items
